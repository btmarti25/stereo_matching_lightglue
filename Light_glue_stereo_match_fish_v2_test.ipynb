{"cells":[{"cell_type":"markdown","metadata":{"id":"c90vjurUwZKz"},"source":["# LightGlue Demo\n","In this notebook we match two pairs of images using LightGlue with early stopping and point pruning."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76252,"status":"ok","timestamp":1716797283196,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"Nqfo374DwZK6","outputId":"de76f228-8f52-40fd-f0ae-92dcfc3c83cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'LightGlue' already exists and is not an empty directory.\n","/Users/benmartin/Library/CloudStorage/GoogleDrive-btmarti25@gmail.com/My Drive/Projects/stereo_matching_lightglue/LightGlue\n"]}],"source":["# If we are on colab: this clones the repo and installs the dependencies\n","from pathlib import Path\n","\n","if Path.cwd().name != \"LightGlue\":\n","    !git clone --quiet https://github.com/cvg/LightGlue/\n","    %cd LightGlue\n","    !pip install --progress-bar off --quiet -e .\n","\n","from lightglue import LightGlue, SuperPoint, DISK\n","from lightglue.utils import load_image, rbd\n","from lightglue import viz2d\n","import torch\n","\n","torch.set_grad_enabled(False)\n","images = Path(\"assets\")\n","\n","\n","## test the LightGlue class  dd"]},{"cell_type":"markdown","metadata":{"id":"bcmXKSMIwZK8"},"source":["## Load extractor and matcher module\n","In this example we use SuperPoint features combined with LightGlue."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1325,"status":"ok","timestamp":1716797284518,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"N9sVgj-fwZK8","outputId":"b45df010-deb7-4b00-8666-20fd7b7a099e"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 'mps', 'cpu'\n","\n","extractor = SuperPoint(max_num_keypoints=2048).eval().to(device)  # load the extractor\n","matcher = LightGlue(features=\"superpoint\").eval().to(device)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QV2qM4yAJNb_"},"outputs":[],"source":["%pip install ultralytics\n","from IPython import display\n","display.clear_output()\n","\n","from ultralytics import YOLO"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40405,"status":"ok","timestamp":1716797324920,"user":{"displayName":"","userId":""},"user_tz":-120},"id":"bvsiakeHwk2a","outputId":"bd99f018-5e1b-49af-fd17-7298c0ab6e6e"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","0: 1088x1920 6 chromiss, 1837.6ms\n","Speed: 26.2ms preprocess, 1837.6ms inference, 4.1ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1680.9ms\n","Speed: 13.7ms preprocess, 1680.9ms inference, 2.8ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1709.1ms\n","Speed: 12.8ms preprocess, 1709.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1691.1ms\n","Speed: 11.5ms preprocess, 1691.1ms inference, 1.4ms postprocess per image at shape (1, 3, 1088, 1920)\n"]},{"name":"stdout","output_type":"stream","text":["Left Tracks: [[          1           0      2401.7      692.83      2542.7      789.43]\n"," [          2           0        3141      161.02      3261.7      231.69]\n"," [          3           0      3357.6      452.89      3513.1       513.6]\n"," [          4           0      1863.2      887.94      1930.4      943.71]\n"," [          5           0      2091.7      1376.9      2249.7      1442.7]\n"," [          6           0        1951      607.78      1987.7      646.18]\n"," [          1           1        2408       698.4        2548      792.16]\n"," [          2           1      3146.1      162.45      3263.2      234.33]\n"," [          3           1      3361.1      452.84      3520.1      514.39]\n"," [          4           1      1864.4      893.04        1933      946.64]\n"," [          5           1      2098.4      1376.1      2261.5      1445.6]\n"," [          6           1      1948.4      608.45      1989.6      645.09]]\n","Right Tracks: [[    0.89709           0      1204.5      507.44      1344.2      603.58]\n"," [    0.89568           0      705.77      1216.8      853.49      1295.8]\n"," [    0.88909           0      2507.2      232.04      2669.1      308.07]\n"," [    0.88638           0      1056.7      721.07      1150.1      779.08]\n"," [    0.86924           0        2785        1005      2951.7      1082.3]\n"," [    0.85665           0      1092.1      403.07      1171.9      445.01]\n"," [    0.37987           0      2634.5      1235.2      2698.2      1322.5]\n"," [          7           1      1210.2       511.4      1354.2      607.09]\n"," [          8           1       712.6        1217      858.63      1297.3]\n"," [          9           1      2511.5      233.13      2673.2      308.95]\n"," [         10           1      1058.7      723.83        1153      780.16]\n"," [         11           1      2775.1      1006.4      2940.1        1086]\n"," [         12           1      1089.9      403.11      1175.3      445.87]]\n"]}],"source":["from sync_tools import calculate_offset_fft\n","from ultralytics import YOLO\n","from track_and_match_tools import get_yolo_tracks_left_right, get_id_matches_lightglue\n","\n","model = YOLO(\"/Users/benmartin/Library/CloudStorage/GoogleDrive-btmarti25@gmail.com/My Drive/best_close_chromis.pt\")\n","video1_path = \"/Volumes/HD_Feeding/30_11_2023/S1_L1/Card_1_left/DCIM/100GOPRO/GX030035.MP4\"\n","video2_path = \"/Volumes/HD_Feeding/30_11_2023/S1_L1/Card_2_right/DCIM/100GOPRO/GX030455.MP4\"\n","\n","#get freames per second of video1\n","import cv2\n","cap = cv2.VideoCapture(video1_path)\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","#offset = int(fps*calculate_offset_fft(video1_path, video2_path))\n","offset = 61\n","start_frame = 100\n","\n","n_frames = 2\n","left_tracks, right_tracks = get_yolo_tracks_left_right(start_frame, n_frames, offset, model, video1_path, video2_path)\n","all_matched_ids = get_id_matches_lightglue(start_frame, offset, n_frames, video1_path, video2_path,left_tracks, right_tracks)\n","\n","print(\"Left Tracks:\", left_tracks)\n","print(\"Right Tracks:\", right_tracks)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","0: 1088x1920 6 chromiss, 1830.9ms\n","Speed: 19.0ms preprocess, 1830.9ms inference, 3.6ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1754.5ms\n","Speed: 13.2ms preprocess, 1754.5ms inference, 2.8ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 1 chromis, 1730.3ms\n","Speed: 14.1ms preprocess, 1730.3ms inference, 1.3ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1763.3ms\n","Speed: 11.9ms preprocess, 1763.3ms inference, 0.9ms postprocess per image at shape (1, 3, 1088, 1920)\n"]},{"ename":"NameError","evalue":"name 'load_image' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      2\u001b[0m left_tracks, right_tracks \u001b[38;5;241m=\u001b[39m get_yolo_tracks_left_right(start_frame, n_frames, offset, model, video1_path, video2_path)\n\u001b[0;32m----> 3\u001b[0m all_matched_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_id_matches_lightglue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo2_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mleft_tracks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_tracks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeft Tracks:\u001b[39m\u001b[38;5;124m\"\u001b[39m, left_tracks)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRight Tracks:\u001b[39m\u001b[38;5;124m\"\u001b[39m, right_tracks)\n","File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-btmarti25@gmail.com/My Drive/Projects/stereo_matching_lightglue/track_and_match_tools.py:131\u001b[0m, in \u001b[0;36mget_id_matches_lightglue\u001b[0;34m(start_frame, offset, n_frames, video1_path, video2_path, left_tracks, right_tracks)\u001b[0m\n\u001b[1;32m    129\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe1.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame1, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR))\n\u001b[1;32m    130\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe2.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame2, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR))\n\u001b[0;32m--> 131\u001b[0m image0 \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe1.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m image1 \u001b[38;5;241m=\u001b[39m load_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe2.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m feats0 \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mextract(image0\u001b[38;5;241m.\u001b[39mto(device))\n","\u001b[0;31mNameError\u001b[0m: name 'load_image' is not defined"]}],"source":["n_frames = 2\n","left_tracks, right_tracks = get_yolo_tracks_left_right(start_frame, n_frames, offset, model, video1_path, video2_path)\n","all_matched_ids = get_id_matches_lightglue(start_frame, offset, n_frames, video1_path, video2_path,left_tracks, right_tracks)\n","\n","print(\"Left Tracks:\", left_tracks)\n","print(\"Right Tracks:\", right_tracks)\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[(6.0, 12.0), (4.0, 10.0), (1.0, 7.0), (3.0, 9.0)]\n"]}],"source":["import numpy as np\n","\n","def find_matches(left_bbs, right_bbs, m_kpts0, m_kpts1):\n","    matches_in_box_left = []\n","    matches_in_box_left_index = []\n","\n","    for bb in left_bbs:\n","        id, frame, x1, y1, x2, y2 = bb\n","        for j, match in enumerate(m_kpts0):\n","            if x1 < match[0] < x2 and y1 < match[1] < y2:\n","                matches_in_box_left.append(id)\n","                matches_in_box_left_index.append(j)\n","\n","    matches_in_box_right = []\n","    matches_in_box_right_index = []\n","\n","    for bb in right_bbs:\n","        id, frame, x1, y1, x2, y2 = bb\n","        for j, match in enumerate(m_kpts1):\n","            if x1 < match[0] < x2 and y1 < match[1] < y2:\n","                matches_in_box_right.append(id)\n","                matches_in_box_right_index.append(j)\n","\n","    # Find common elements\n","    common_elements = set(matches_in_box_left_index).intersection(set(matches_in_box_right_index))\n","\n","    # Create a table to store the indices\n","    table = []\n","\n","    # Find the indices of common elements in both lists\n","    for element in common_elements:\n","        indices_list1 = [i for i, x in enumerate(matches_in_box_left_index) if x == element]\n","        indices_list2 = [i for i, x in enumerate(matches_in_box_right_index) if x == element]\n","        for index1 in indices_list1:\n","            for index2 in indices_list2:\n","                table.append({'Value': element, 'Index in list1': index1, 'Index in list2': index2})\n","\n","    # Extract indices into NumPy arrays\n","    left_ids = np.array([row['Index in list1'] for row in table], dtype=int)\n","    right_ids = np.array([row['Index in list2'] for row in table], dtype=int)\n","\n","    # Convert lists to NumPy arrays\n","    matches_in_box_right = np.array(matches_in_box_right)\n","    matches_in_box_left = np.array(matches_in_box_left)\n","\n","    # Use a set to store unique pairs\n","    unique_matches = set((matches_in_box_left[left_id], matches_in_box_right[right_id])\n","                         for left_id, right_id in zip(left_ids, right_ids))\n","\n","    # Convert the set back to a list\n","    matched_data = list(unique_matches)\n","\n","    return matched_data\n","\n","# Example usage:\n","\n","\n","matched_data = find_matches(left_bbs, right_bbs, m_kpts0, m_kpts1)\n","print(matched_data)\n"]},{"cell_type":"markdown","metadata":{"id":"adaXxTnjwZLL"},"source":["## Easy example\n","The top image shows the matches, while the bottom image shows the point pruning across layers. In this case, LightGlue prunes a few points with occlusions, but is able to stop the context aggregation after 4/9 layers."]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","def get_yolo_tracks_left_right(start_frame, n_frames, offset, model, video1_path, video2_path):\n","    # Initialize video capture\n","    cap1 = cv2.VideoCapture(video1_path)\n","    cap1.set(cv2.CAP_PROP_POS_FRAMES, start_frame + offset)\n","    \n","    cap2 = cv2.VideoCapture(video2_path)\n","    cap2.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","\n","    left_tracks = []\n","    right_tracks = []\n","\n","    # Process left video\n","    for i in range(n_frames):\n","        ret, frame = cap1.read()\n","        if not ret:\n","            break\n","        results_left = model.track(source=frame, persist=True)\n","        \n","        if results_left[0].boxes is not None:\n","            for box in results_left[0].boxes.data:\n","                x1, y1, x2, y2, id, conf = box[0:6].detach().cpu().numpy()\n","                frame_number = i\n","                left_tracks.append([id, frame_number, x1, y1, x2, y2])\n","\n","    # Process right video\n","    for i in range(n_frames):\n","        ret, frame = cap2.read()\n","        if not ret:\n","            break\n","        results_right = model.track(source=frame, persist=True)\n","        \n","        if results_right[0].boxes is not None:\n","            for box in results_right[0].boxes.data:\n","                x1, y1, x2, y2, id, conf = box[0:6].detach().cpu().numpy()\n","                frame_number = i\n","                right_tracks.append([id, frame_number, x1, y1, x2, y2])\n","\n","    # Convert lists to numpy arrays\n","    left_tracks = np.array(left_tracks)\n","    right_tracks = np.array(right_tracks)\n","\n","    return left_tracks, right_tracks\n","\n"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","def get_id_matches_lightglue(start_frame, offset, n_frames, video1_path, video2_path,left_tracks, right_tracks):\n","    # Initialize video capture\n","    cap1 = cv2.VideoCapture(video1_path)\n","    cap1.set(cv2.CAP_PROP_POS_FRAMES, offset + start_frame)\n","    \n","    cap2 = cv2.VideoCapture(video2_path)\n","    cap2.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","    \n","    # Assume these are provided by your environment\n","    # rt = np.array(right_tracks)  # Right bounding boxes with tracking data\n","    # lt = np.array(left_tracks)   # Left bounding boxes with tracking data\n","\n","    # You would populate rt and lt with your tracking data before this function call\n","    rt = np.array(right_tracks)  # Replace with actual right tracking data\n","    lt = np.array(left_tracks)   # Replace with actual left tracking data\n","\n","    all_matched_ids = []\n","\n","    for i in range(n_frames):\n","        ret1, frame1 = cap1.read()\n","        ret2, frame2 = cap2.read()\n","        if not ret1 or not ret2:\n","            break\n","\n","        # Save the frames temporarily\n","        cv2.imwrite(\"frame1.png\", cv2.cvtColor(frame1, cv2.COLOR_RGB2BGR))\n","        cv2.imwrite(\"frame2.png\", cv2.cvtColor(frame2, cv2.COLOR_RGB2BGR))\n","        image0 = load_image(\"frame1.png\")\n","        image1 = load_image(\"frame2.png\")\n","\n","        feats0 = extractor.extract(image0.to(device))\n","        feats1 = extractor.extract(image1.to(device))\n","        matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","        feats0, feats1, matches01 = [\n","            rbd(x) for x in [feats0, feats1, matches01]\n","        ]  # remove batch dimension\n","\n","        kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","        m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","\n","        # Get bounding boxes for the current frame\n","        left_bbs = lt[lt[:, 1] == i, :]\n","        right_bbs = rt[rt[:, 1] == i, :]\n","\n","        matched_ids = find_matches(left_bbs, right_bbs, m_kpts0, m_kpts1)\n","        \n","        # Add the frame number to each match\n","        matched_ids_with_frame = [(i, match[0], match[1]) for match in matched_ids]\n","\n","        # Append the matches to the data structure\n","        all_matched_ids.extend(matched_ids_with_frame)\n","\n","    # Convert to numpy array\n","    all_matched_ids = np.array(all_matched_ids)\n","    \n","    # Remove rows with non-integer values\n","    all_matched_ids = all_matched_ids[\n","        np.all(all_matched_ids[:, 1:].astype(float) == all_matched_ids[:, 1:].astype(int), axis=1)\n","    ]\n","    \n","    return np.array(all_matched_ids)\n","\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","0: 1088x1920 6 chromiss, 1663.8ms\n","Speed: 15.7ms preprocess, 1663.8ms inference, 3.5ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1643.8ms\n","Speed: 13.5ms preprocess, 1643.8ms inference, 3.2ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1648.7ms\n","Speed: 11.8ms preprocess, 1648.7ms inference, 4.1ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1724.1ms\n","Speed: 12.8ms preprocess, 1724.1ms inference, 3.3ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1752.4ms\n","Speed: 12.1ms preprocess, 1752.4ms inference, 1.2ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1832.4ms\n","Speed: 13.2ms preprocess, 1832.4ms inference, 3.3ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1780.2ms\n","Speed: 14.8ms preprocess, 1780.2ms inference, 2.5ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1783.4ms\n","Speed: 12.5ms preprocess, 1783.4ms inference, 4.1ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1730.3ms\n","Speed: 13.9ms preprocess, 1730.3ms inference, 4.2ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1943.6ms\n","Speed: 12.7ms preprocess, 1943.6ms inference, 3.5ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1788.5ms\n","Speed: 12.8ms preprocess, 1788.5ms inference, 1.1ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1779.5ms\n","Speed: 14.3ms preprocess, 1779.5ms inference, 1.9ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1712.6ms\n","Speed: 54.0ms preprocess, 1712.6ms inference, 4.0ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1637.2ms\n","Speed: 11.4ms preprocess, 1637.2ms inference, 1.3ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1641.7ms\n","Speed: 10.5ms preprocess, 1641.7ms inference, 1.2ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1656.5ms\n","Speed: 12.4ms preprocess, 1656.5ms inference, 1.1ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1656.8ms\n","Speed: 12.0ms preprocess, 1656.8ms inference, 3.1ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1668.9ms\n","Speed: 11.9ms preprocess, 1668.9ms inference, 3.8ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1886.0ms\n","Speed: 12.2ms preprocess, 1886.0ms inference, 4.2ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1957.8ms\n","Speed: 12.8ms preprocess, 1957.8ms inference, 3.5ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 7 chromiss, 1739.1ms\n","Speed: 13.1ms preprocess, 1739.1ms inference, 4.4ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1855.6ms\n","Speed: 16.2ms preprocess, 1855.6ms inference, 4.2ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1811.8ms\n","Speed: 13.8ms preprocess, 1811.8ms inference, 1.2ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1747.7ms\n","Speed: 12.3ms preprocess, 1747.7ms inference, 1.1ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1664.1ms\n","Speed: 14.7ms preprocess, 1664.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1675.8ms\n","Speed: 13.5ms preprocess, 1675.8ms inference, 1.1ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1644.6ms\n","Speed: 13.8ms preprocess, 1644.6ms inference, 4.4ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1707.2ms\n","Speed: 12.1ms preprocess, 1707.2ms inference, 3.9ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1668.0ms\n","Speed: 15.1ms preprocess, 1668.0ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1634.8ms\n","Speed: 11.7ms preprocess, 1634.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1675.9ms\n","Speed: 24.8ms preprocess, 1675.9ms inference, 4.0ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1647.1ms\n","Speed: 11.4ms preprocess, 1647.1ms inference, 2.9ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1655.7ms\n","Speed: 11.8ms preprocess, 1655.7ms inference, 4.0ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1651.6ms\n","Speed: 14.4ms preprocess, 1651.6ms inference, 1.7ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1660.2ms\n","Speed: 11.5ms preprocess, 1660.2ms inference, 3.9ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1944.6ms\n","Speed: 11.8ms preprocess, 1944.6ms inference, 2.7ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1690.4ms\n","Speed: 13.1ms preprocess, 1690.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1748.5ms\n","Speed: 14.0ms preprocess, 1748.5ms inference, 3.8ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1826.2ms\n","Speed: 16.9ms preprocess, 1826.2ms inference, 3.2ms postprocess per image at shape (1, 3, 1088, 1920)\n","\n","0: 1088x1920 6 chromiss, 1787.6ms\n","Speed: 12.4ms preprocess, 1787.6ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n"]},{"ename":"NameError","evalue":"name 'find_unique_matches' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[45], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m n_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      6\u001b[0m left_tracks, right_tracks \u001b[38;5;241m=\u001b[39m get_yolo_tracks_left_right(start_frame, n_frames, offset, model, video1_path, video2_path)\n\u001b[0;32m----> 7\u001b[0m all_matched_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_id_matches_lightglue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo2_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mleft_tracks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_tracks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeft Tracks:\u001b[39m\u001b[38;5;124m\"\u001b[39m, left_tracks)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRight Tracks:\u001b[39m\u001b[38;5;124m\"\u001b[39m, right_tracks)\n","Cell \u001b[0;32mIn[44], line 48\u001b[0m, in \u001b[0;36mget_id_matches_lightglue\u001b[0;34m(start_frame, offset, n_frames, video1_path, video2_path, left_tracks, right_tracks)\u001b[0m\n\u001b[1;32m     45\u001b[0m left_bbs \u001b[38;5;241m=\u001b[39m lt[lt[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m i, :]\n\u001b[1;32m     46\u001b[0m right_bbs \u001b[38;5;241m=\u001b[39m rt[rt[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m i, :]\n\u001b[0;32m---> 48\u001b[0m matched_ids \u001b[38;5;241m=\u001b[39m \u001b[43mfind_unique_matches\u001b[49m(left_bbs, right_bbs, m_kpts0, m_kpts1)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Add the frame number to each match\u001b[39;00m\n\u001b[1;32m     51\u001b[0m matched_ids_with_frame \u001b[38;5;241m=\u001b[39m [(i, match[\u001b[38;5;241m0\u001b[39m], match[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matched_ids]\n","\u001b[0;31mNameError\u001b[0m: name 'find_unique_matches' is not defined"]}],"source":["# Example usage\n","\n","start_frame = 100\n","offset = 61\n","n_frames = 20\n","left_tracks, right_tracks = get_yolo_tracks_left_right(start_frame, n_frames, offset, model, video1_path, video2_path)\n","all_matched_ids = get_id_matches_lightglue(start_frame, offset, n_frames, video1_path, video2_path,left_tracks, right_tracks)\n","\n","print(\"Left Tracks:\", left_tracks)\n","print(\"Right Tracks:\", right_tracks)\n"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["all_matched_ids = get_id_matches_lightglue(start_frame, offset, n_frames, video1_path, video2_path,left_tracks, right_tracks)\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[          1          44          51]\n"," [          1          42          49]\n"," [          1          45          52]\n"," [          2          44          51]\n"," [          2          45          52]\n"," [          2          42          49]\n"," [          2          47          54]\n"," [          3          44          51]\n"," [          3          45          52]\n"," [          3          47          54]\n"," [          4          44          51]\n"," [          4          45          52]\n"," [          4          47          54]\n"," [          5          44          51]\n"," [          5          45          52]\n"," [          5          47          54]\n"," [          6          44          51]\n"," [          6          47          54]\n"," [          7          44          51]\n"," [          7          47          54]\n"," [          8          44          51]\n"," [          8          47          54]\n"," [          9          44          51]\n"," [          9          47          54]\n"," [         10          44          51]\n"," [         10          47          54]\n"," [         11          44          51]\n"," [         11          47          54]\n"," [         12          44          51]\n"," [         12          47          54]\n"," [         13          44          51]\n"," [         13          47          54]\n"," [         14          44          51]\n"," [         14          42          49]\n"," [         15          44          51]\n"," [         15          42          49]\n"," [         15          47          54]\n"," [         16          44          51]\n"," [         16          42          49]\n"," [         16          47          54]\n"," [         17          44          51]\n"," [         17          47          54]\n"," [         18          44          51]\n"," [         18          47          54]\n"," [         19          44          51]\n"," [         19          47          54]]\n"]}],"source":["print(all_matched_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[          0           1     0.89709]\n"," [          0           4     0.88638]\n"," [          0           6     0.85665]\n"," [          0           3     0.88909]\n"," [          1           4          11]\n"," [          1           1           8]\n"," [          1           3          10]\n"," [          2           4          11]\n"," [          2           6          13]\n"," [          2           1           8]\n"," [          2           3          10]\n"," [          3           4          11]\n"," [          3           6          13]\n"," [          3           3          10]\n"," [          4           4          11]\n"," [          4           6          13]\n"," [          4           3          10]\n"," [          5           4          11]\n"," [          5           6          13]\n"," [          5           3          10]\n"," [          6           6          13]\n"," [          6           3          10]\n"," [          7           6          13]\n"," [          7           3          10]\n"," [          8           6          13]\n"," [          8           3          10]\n"," [          9           6          13]\n"," [          9           3          10]]\n"]}],"source":["import numpy as np\n","\n","\n","cap1 = cv2.VideoCapture(video1_path)\n","cap1.set(cv2.CAP_PROP_POS_FRAMES, offset+ start_frame)\n","\n","cap2 = cv2.VideoCapture(video2_path)\n","cap2.set(cv2.CAP_PROP_POS_FRAMES, 0 + start_frame)\n","\n","rt = np.array(right_tracks)\n","lt = np.array(left_tracks)\n","\n","all_matched_ids = []\n","# Assuming cap1 and cap2 are your video capture objects\n","for i in range(n_frames):\n","    ret, frame1 = cap1.read()\n","    ret, frame2 = cap2.read()\n","\n","    # Save the frames\n","    cv2.imwrite(\"frame1.png\", cv2.cvtColor(frame1, cv2.COLOR_RGB2BGR))\n","    cv2.imwrite(\"frame2.png\", cv2.cvtColor(frame2, cv2.COLOR_RGB2BGR))\n","    image0 = load_image(\"frame1.png\")\n","    image1 = load_image(\"frame2.png\")\n","\n","    feats0 = extractor.extract(image0.to(device))\n","    feats1 = extractor.extract(image1.to(device))\n","    matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","    feats0, feats1, matches01 = [\n","        rbd(x) for x in [feats0, feats1, matches01]\n","    ]  # remove batch dimension\n","\n","    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","\n","    # Get bounding boxes for the current frame\n","    left_bbs = lt[lt[:, 1] == i, :]\n","    right_bbs = rt[rt[:, 1] == i, :]\n","\n","    matched_ids = find_matches(left_bbs, right_bbs, m_kpts0, m_kpts1)\n","    \n","    # Add the frame number to each match\n","    matched_ids_with_frame = [(i, match[0], match[1]) for match in matched_ids]\n","\n","    # Append the matches to the data structure\n","    all_matched_ids.extend(matched_ids_with_frame)\n","\n","# Print or process the matched IDs for all frames\n","print(np.array(all_matched_ids))\n","\n","    \n","\n","\n","    \n","    "]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"https://github.com/cvg/LightGlue/blob/main/demo.ipynb","timestamp":1716802186460}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
